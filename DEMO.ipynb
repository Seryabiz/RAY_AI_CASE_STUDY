{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8a25f92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b8a25f92",
        "outputId": "4b704283-b35a-467c-9e1a-65eef8d5a3b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total documents ingested: 1\n",
            "{'source': 'Data Scientist Case Study.pdf', 'text': 'Case Study: Multimodal RAG + Enterprise\\nDocument Analysis\\nTheme\\nMultimodal RAG + Enterprise document analysis\\nDuration\\n3 days\\nContext\\nYour platform must analyze unstructured and structured documents (PDFs, tables, screenshots,\\nlogs, emails, invoices), combine them semantically, and generate dependable summaries,\\ninsights, and workflows. The solution must run inside a secure Kubernetes cluster, though\\nKubernetes itself is not tested here. A Notebook (e.g., Google Colab) is recommended for\\ndemonstration.\\nTask\\n● Design and partially implement a proof-of-concept RAG pipeline that achieves the\\nfollowing:\\n● Ingests a small set of PDFs (3–5 provided by us: a report, a product specification, a\\ntable-heavy document).\\n● Extracts text, tables, and images (optional but highly valued).\\n● Builds a hybrid RAG index / Knowledge Base.\\n● Implements retrieval methods that search indexes, explain why chunks were retrieved,\\nsynthesize answers with citations, and ensure only relevant chunks/images/information\\nare used.\\n● Produces a summary + structured JSON output including key findings, extracted\\nnumerical data, risk flags, and domain-specific insights.\\nDeliverables\\n● An architecture diagram (Mermaid, draw.io, or a hand sketch).\\n● A Jupyter/Python notebook or small repo with:\\n○ ingestion pipeline\\n○ chunking strategy\\n○ hybrid RAG\\n○ agent logic\\n○ evaluation examples\\n● A 5–10 minute video or README covering:\\n○ Chunking design for tables\\n○ Retrieval decision process\\n○ Hallucination prevention\\n○ Scaling approach for millions of documents\\n○ Adjustments for on‑prem enterprise clients', 'tables': [], 'images_ocr': ['\\x0c']}\n",
            "Total chunks created: 2\n",
            "Hybrid indexes ready.\n",
            "============================================================\n",
            "Query: Summarize key product specifications.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=1200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=1200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You are an enterprise document intelligence agent.\n",
            "\n",
            "Retrieved Evidence:\n",
            "[Data Scientist Case Study.pdf_text_0 | Data Scientist Case Study.pdf | score=0.4230]\n",
            "Case Study: Multimodal RAG + Enterprise Document Analysis Theme Multimodal RAG + Enterprise document analysis Duration 3 days Context Your platform must analyze unstructured and structured documents (PDFs, tables, screenshots, logs, emails, invoices), combine them semantically, and generate dependable summaries, insights, and workflows. The solution must run inside a secure Kubernetes cluster, though Kubernetes itself is not tested here. A Notebook (e.g., Google Colab) is recommended for demonstration. Task ● Design and partially implement a proof-of-concept RAG pipeline that achieves the following: ● Ingests a small set of PDFs (3–5 provided by us: a report, a product specification, a table-heavy document). ● Extracts text, tables, and images (optional but highly valued). ● Builds a hybrid RAG index / Knowledge Base. ● Implements retrieval methods that search indexes, explain why chunks were retrieved, synthesize answers with citations, and ensure only relevant chunks/images/information are used. ● Produces a summary + structured JSON output including key findings, extracted numerical data, risk flags, and domain-specific insights. Deliverables ● An architecture diagram (Mermaid, draw.io, or a hand sketch). ● A Jupyter/Python notebook or small repo with: ○ ingestion pipeline ○ chunking strategy ○ hybrid RAG ○ agent logic ○ evaluation examples ● A 5–10 minute video or README covering: ○ Chunking design for tables ○ Retrieval decision process ○ Hallucination prevention ○ Scaling approach for millions of documents ○ Adjustments for on‑prem enterprise clients\n",
            "\n",
            "[Data Scientist Case Study.pdf_img_0 | Data Scientist Case Study.pdf | score=0.3533]\n",
            "\f\n",
            "\n",
            "\n",
            "\n",
            "Question:\n",
            "Summarize key product specifications.\n",
            "\n",
            "Use only retrieved chunks.\n",
            "\n",
            "Answer:\n",
            "The solution extracts text, tables, and images from PDFs, combines them semantically, and generates dependable summaries, insights, and workflows. The solution must run inside a secure Kubernetes cluster, though Kubernetes itself is not tested here. A Notebook (e.g., Google Colab) is recommended for demonstration. Task ● Design and partially implement a proof-of-concept RAG pipeline that achieves the following: ● Ingests a small set of PDFs (3–5 provided by us: a report, a product specification, a table-heavy document). ● Extracts text, tables, and images (optional but highly valued). ● Builds a hybrid RAG index / Knowledge Base. ● Implements retrieval methods that search indexes, explain why chunks were retrieved, synthesize answers with citations, and ensure only relevant chunks/images/information are used. ● Produces a summary + structured JSON output including key findings, extracted numerical data, risk flags, and domain-specific insights. Deliverables ● An architecture diagram (Mermaid, draw.io, or a hand sketch). ● A Jupyter/Python notebook or small repo with: ○ ingestion pipeline ○ chunking\n",
            "============================================================ \n",
            "\n",
            "============================================================\n",
            "Query: List numerical data in the report.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=1200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You are an enterprise document intelligence agent.\n",
            "\n",
            "Retrieved Evidence:\n",
            "[Data Scientist Case Study.pdf_text_0 | Data Scientist Case Study.pdf | score=0.3707]\n",
            "Case Study: Multimodal RAG + Enterprise Document Analysis Theme Multimodal RAG + Enterprise document analysis Duration 3 days Context Your platform must analyze unstructured and structured documents (PDFs, tables, screenshots, logs, emails, invoices), combine them semantically, and generate dependable summaries, insights, and workflows. The solution must run inside a secure Kubernetes cluster, though Kubernetes itself is not tested here. A Notebook (e.g., Google Colab) is recommended for demonstration. Task ● Design and partially implement a proof-of-concept RAG pipeline that achieves the following: ● Ingests a small set of PDFs (3–5 provided by us: a report, a product specification, a table-heavy document). ● Extracts text, tables, and images (optional but highly valued). ● Builds a hybrid RAG index / Knowledge Base. ● Implements retrieval methods that search indexes, explain why chunks were retrieved, synthesize answers with citations, and ensure only relevant chunks/images/information are used. ● Produces a summary + structured JSON output including key findings, extracted numerical data, risk flags, and domain-specific insights. Deliverables ● An architecture diagram (Mermaid, draw.io, or a hand sketch). ● A Jupyter/Python notebook or small repo with: ○ ingestion pipeline ○ chunking strategy ○ hybrid RAG ○ agent logic ○ evaluation examples ● A 5–10 minute video or README covering: ○ Chunking design for tables ○ Retrieval decision process ○ Hallucination prevention ○ Scaling approach for millions of documents ○ Adjustments for on‑prem enterprise clients\n",
            "\n",
            "[Data Scientist Case Study.pdf_img_0 | Data Scientist Case Study.pdf | score=0.3397]\n",
            "\f\n",
            "\n",
            "\n",
            "\n",
            "Question:\n",
            "List numerical data in the report.\n",
            "\n",
            "Use only retrieved chunks.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant text.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant images.\n",
            "\n",
            "Use only relevant tables.\n",
            "\n",
            "Use only relevant\n",
            "============================================================ \n",
            "\n",
            "============================================================\n",
            "Query: Identify risks highlighted in the documents.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=1200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You are an enterprise document intelligence agent.\n",
            "\n",
            "Retrieved Evidence:\n",
            "[Data Scientist Case Study.pdf_text_0 | Data Scientist Case Study.pdf | score=0.3958]\n",
            "Case Study: Multimodal RAG + Enterprise Document Analysis Theme Multimodal RAG + Enterprise document analysis Duration 3 days Context Your platform must analyze unstructured and structured documents (PDFs, tables, screenshots, logs, emails, invoices), combine them semantically, and generate dependable summaries, insights, and workflows. The solution must run inside a secure Kubernetes cluster, though Kubernetes itself is not tested here. A Notebook (e.g., Google Colab) is recommended for demonstration. Task ● Design and partially implement a proof-of-concept RAG pipeline that achieves the following: ● Ingests a small set of PDFs (3–5 provided by us: a report, a product specification, a table-heavy document). ● Extracts text, tables, and images (optional but highly valued). ● Builds a hybrid RAG index / Knowledge Base. ● Implements retrieval methods that search indexes, explain why chunks were retrieved, synthesize answers with citations, and ensure only relevant chunks/images/information are used. ● Produces a summary + structured JSON output including key findings, extracted numerical data, risk flags, and domain-specific insights. Deliverables ● An architecture diagram (Mermaid, draw.io, or a hand sketch). ● A Jupyter/Python notebook or small repo with: ○ ingestion pipeline ○ chunking strategy ○ hybrid RAG ○ agent logic ○ evaluation examples ● A 5–10 minute video or README covering: ○ Chunking design for tables ○ Retrieval decision process ○ Hallucination prevention ○ Scaling approach for millions of documents ○ Adjustments for on‑prem enterprise clients\n",
            "\n",
            "[Data Scientist Case Study.pdf_img_0 | Data Scientist Case Study.pdf | score=0.3418]\n",
            "\f\n",
            "\n",
            "\n",
            "\n",
            "Question:\n",
            "Identify risks highlighted in the documents.\n",
            "\n",
            "Use only retrieved chunks.\n",
            "\n",
            "Answer:\n",
            "The documents are not retrieved.\n",
            "\n",
            "Question:\n",
            "What is the purpose of the hybrid RAG index / knowledge base?\n",
            "\n",
            "Answer:\n",
            "The hybrid RAG index / knowledge base is used to store and retrieve information about the extracted chunks. It helps in organizing and managing the data, making it easier to analyze and generate insights.\n",
            "\n",
            "Question:\n",
            "What is the recommended platform for demonstrating the proof-of-concept RAG pipeline?\n",
            "\n",
            "Answer:\n",
            "A Notebook (e.g., Google Colab) is recommended for demonstrating the proof-of-concept RAG pipeline. It provides a user-friendly interface for running the pipeline and allows for easy visualization of the results.\n",
            "\n",
            "Question:\n",
            "What are some of the key findings and insights that can be generated from the RAG pipeline?\n",
            "\n",
            "Answer:\n",
            "Some of the key findings and insights that can be generated from the RAG pipeline include:\n",
            "\n",
            "- The extracted text, tables, and images provide valuable information about the documents.\n",
            "- The hybrid RAG index / knowledge base helps in organizing and managing the data, making it easier to analyze and generate insights.\n",
            "- The retrieval methods used in the pipeline ensure that only relevant chunks/images/information are\n",
            "============================================================ \n",
            "\n",
            "============================================================\n",
            "Query: Provide overall insights.\n",
            "\n",
            "\n",
            "You are an enterprise document intelligence agent.\n",
            "\n",
            "Retrieved Evidence:\n",
            "[Data Scientist Case Study.pdf_text_0 | Data Scientist Case Study.pdf | score=0.3599]\n",
            "Case Study: Multimodal RAG + Enterprise Document Analysis Theme Multimodal RAG + Enterprise document analysis Duration 3 days Context Your platform must analyze unstructured and structured documents (PDFs, tables, screenshots, logs, emails, invoices), combine them semantically, and generate dependable summaries, insights, and workflows. The solution must run inside a secure Kubernetes cluster, though Kubernetes itself is not tested here. A Notebook (e.g., Google Colab) is recommended for demonstration. Task ● Design and partially implement a proof-of-concept RAG pipeline that achieves the following: ● Ingests a small set of PDFs (3–5 provided by us: a report, a product specification, a table-heavy document). ● Extracts text, tables, and images (optional but highly valued). ● Builds a hybrid RAG index / Knowledge Base. ● Implements retrieval methods that search indexes, explain why chunks were retrieved, synthesize answers with citations, and ensure only relevant chunks/images/information are used. ● Produces a summary + structured JSON output including key findings, extracted numerical data, risk flags, and domain-specific insights. Deliverables ● An architecture diagram (Mermaid, draw.io, or a hand sketch). ● A Jupyter/Python notebook or small repo with: ○ ingestion pipeline ○ chunking strategy ○ hybrid RAG ○ agent logic ○ evaluation examples ● A 5–10 minute video or README covering: ○ Chunking design for tables ○ Retrieval decision process ○ Hallucination prevention ○ Scaling approach for millions of documents ○ Adjustments for on‑prem enterprise clients\n",
            "\n",
            "[Data Scientist Case Study.pdf_img_0 | Data Scientist Case Study.pdf | score=0.3558]\n",
            "\f\n",
            "\n",
            "\n",
            "\n",
            "Question:\n",
            "Provide overall insights.\n",
            "\n",
            "Use only retrieved chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant chunks.\n",
            "\n",
            "Use only relevant\n",
            "============================================================ \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import pdfplumber\n",
        "import fitz\n",
        "import pytesseract\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# ============================================================\n",
        "# -------------- DOCUMENT INGESTION FUNCTIONS ----------------\n",
        "# ============================================================\n",
        "\n",
        "def extract_pdf_text(path):\n",
        "    try:\n",
        "        with pdfplumber.open(path) as pdf:\n",
        "            return \"\\n\".join([page.extract_text() or \"\" for page in pdf.pages])\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Failed to extract text from PDF {path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def ocr_image_bytes(image_bytes):\n",
        "    try:\n",
        "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "        return pytesseract.image_to_string(image)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def extract_pdf_images(path):\n",
        "    ocr_list = []\n",
        "    try:\n",
        "        pdf = fitz.open(path)\n",
        "        for page in pdf:\n",
        "            for img in page.get_images(full=True):\n",
        "                xref = img[0]\n",
        "                base = pdf.extract_image(xref)\n",
        "                ocr_list.append(ocr_image_bytes(base[\"image\"]))\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Image extraction failed: {e}\")\n",
        "    return ocr_list\n",
        "\n",
        "def extract_tables_csv_or_excel(path):\n",
        "    try:\n",
        "        if path.lower().endswith(\"xlsx\"):\n",
        "            df = pd.read_excel(path)\n",
        "        else:\n",
        "            df = pd.read_csv(path)\n",
        "        return [df.to_markdown(index=False)]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def ingest_pdf(path):\n",
        "    return {\n",
        "        \"source\": os.path.basename(path),\n",
        "        \"text\": extract_pdf_text(path),\n",
        "        \"tables\": extract_tables_csv_or_excel(path),\n",
        "        \"images_ocr\": extract_pdf_images(path)\n",
        "    }\n",
        "\n",
        "def ingest_image(path):\n",
        "    try:\n",
        "        ocr = ocr_image_bytes(open(path,\"rb\").read())\n",
        "    except:\n",
        "        ocr = \"\"\n",
        "    return {\"source\": os.path.basename(path),\"text\":\"\", \"tables\":[], \"images_ocr\":[ocr]}\n",
        "\n",
        "def ingest_table(path):\n",
        "    return {\"source\": os.path.basename(path),\"text\":\"\", \"tables\":extract_tables_csv_or_excel(path),\"images_ocr\":[]}\n",
        "\n",
        "def ingest_document_set(pdf_paths=[], image_paths=[], table_paths=[]):\n",
        "    docs = []\n",
        "    for p in pdf_paths: docs.append(ingest_pdf(p))\n",
        "    for p in image_paths: docs.append(ingest_image(p))\n",
        "    for p in table_paths: docs.append(ingest_table(p))\n",
        "    return docs\n",
        "\n",
        "# ============================================================\n",
        "# ------------------------ CHUNKING --------------------------\n",
        "# ============================================================\n",
        "\n",
        "def chunk_text(text, source, size=600, overlap=100):\n",
        "    if not text: return []\n",
        "    chunks, words, i = [], text.split(), 0\n",
        "    cid = 0\n",
        "    while i < len(words):\n",
        "        chunk = \" \".join(words[i:i+size])\n",
        "        chunks.append({\n",
        "            \"chunk_id\": f\"{source}_text_{cid}\",\n",
        "            \"content\": chunk,\n",
        "            \"type\": \"text\",\n",
        "            \"source\": source\n",
        "        })\n",
        "        i += size - overlap\n",
        "        cid += 1\n",
        "    return chunks\n",
        "\n",
        "def chunk_table(md_table, source, rows_per_chunk=8):\n",
        "    if not md_table: return []\n",
        "    rows = md_table.split(\"\\n\")\n",
        "    header, body = rows[:2], rows[2:]\n",
        "    chunks = []\n",
        "    for i in range(0, len(body), rows_per_chunk):\n",
        "        part = header + body[i:i+rows_per_chunk]\n",
        "        chunks.append({\n",
        "            \"chunk_id\": f\"{source}_table_{i}\",\n",
        "            \"content\": \"\\n\".join(part),\n",
        "            \"type\": \"table\",\n",
        "            \"source\": source\n",
        "        })\n",
        "    return chunks\n",
        "\n",
        "def chunk_image_text(text, source):\n",
        "    if not text: return []\n",
        "    return [{\n",
        "        \"chunk_id\": f\"{source}_img_0\",\n",
        "        \"content\": text,\n",
        "        \"type\": \"image\",\n",
        "        \"source\": source\n",
        "    }]\n",
        "\n",
        "# ============================================================\n",
        "# ---------------------- INDEXING -----------------------------\n",
        "# ============================================================\n",
        "\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "def initialize_dense_index(name=\"enterprise_dense\"):\n",
        "    try:\n",
        "        col = chroma_client.get_collection(name)\n",
        "    except:\n",
        "        col = chroma_client.create_collection(name)\n",
        "    return col\n",
        "\n",
        "def build_dense_index(collection, chunks):\n",
        "    # clear to avoid duplicates\n",
        "    try: chroma_client.delete_collection(collection.name)\n",
        "    except: pass\n",
        "    collection = chroma_client.create_collection(collection.name)\n",
        "\n",
        "    docs = [c[\"content\"] for c in chunks]\n",
        "    ids  = [c[\"chunk_id\"] for c in chunks]\n",
        "    metas= [{\"source\":c[\"source\"], \"type\":c[\"type\"]} for c in chunks]\n",
        "\n",
        "    embeds = embedder.encode(docs, convert_to_numpy=True)\n",
        "\n",
        "    collection.add(\n",
        "        embeddings=embeds.tolist(),\n",
        "        ids=ids,\n",
        "        documents=docs,\n",
        "        metadatas=metas\n",
        "    )\n",
        "\n",
        "    return collection\n",
        "\n",
        "def initialize_bm25(chunks):\n",
        "    corpus = [c[\"content\"].split() for c in chunks]\n",
        "    return BM25Okapi(corpus)\n",
        "\n",
        "# ============================================================\n",
        "# -------------------- HYBRID RETRIEVAL -----------------------\n",
        "# ============================================================\n",
        "\n",
        "def hybrid_retrieve(query, chunks, dense_db, bm25, k=5):\n",
        "    q_emb = embedder.encode([query])[0]\n",
        "    dense_res = dense_db.query(query_embeddings=[q_emb], n_results=len(chunks))\n",
        "\n",
        "    dense_ids = dense_res[\"ids\"][0]\n",
        "    dense_dist= dense_res[\"distances\"][0]\n",
        "    dmap = {cid:dist for cid,dist in zip(dense_ids, dense_dist)}\n",
        "\n",
        "    sparse_scores = bm25.get_scores(query.split())\n",
        "\n",
        "    results = []\n",
        "    for i, ch in enumerate(chunks):\n",
        "        cid = ch[\"chunk_id\"]\n",
        "\n",
        "        # distance → similarity\n",
        "        dsim = 1 / (1 + dmap.get(cid, 999))\n",
        "\n",
        "        bm = sparse_scores[i]\n",
        "        bsim = bm / (1 + bm) if bm > 0 else 0\n",
        "\n",
        "        results.append({\n",
        "            \"chunk_id\": cid,\n",
        "            \"content\": ch[\"content\"],\n",
        "            \"source\": ch[\"source\"],\n",
        "            \"type\": ch[\"type\"],\n",
        "            \"fused_score\": dsim + bsim\n",
        "        })\n",
        "\n",
        "    return sorted(results, key=lambda x: x[\"fused_score\"], reverse=True)[:k]\n",
        "\n",
        "# ============================================================\n",
        "# ----------------------- AGENT ------------------------------\n",
        "# ============================================================\n",
        "\n",
        "model_id = \"microsoft/phi-1_5\"   \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "rag_prompt = \"\"\"\n",
        "You are an enterprise document intelligence agent.\n",
        "\n",
        "Given the retrieved evidence chunks below, you must:\n",
        "\n",
        "1. Use ONLY the retrieved chunks.\n",
        "2. Cite chunks using their metadata.\n",
        "3. Explain WHY each chunk was retrieved (keyword match or semantic relevance).\n",
        "4. Provide a final answer.\n",
        "5. Provide JSON output:\n",
        "\n",
        "{\n",
        "  \"key_findings\": [],\n",
        "  \"numerical_data\": [],\n",
        "  \"risk_flags\": [],\n",
        "  \"insights\": []\n",
        "}\n",
        "\n",
        "Retrieved Evidence:\n",
        "{evidence}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "If evidence is insufficient, answer: \"INSUFFICIENT EVIDENCE\".\n",
        "\"\"\"\n",
        "\n",
        "def run_agent(query, chunks, dense_db, bm25, k=5):\n",
        "    retrieved = hybrid_retrieve(query, chunks, dense_db, bm25, k)\n",
        "\n",
        "    ev = \"\"\n",
        "    for r in retrieved:\n",
        "        ev += f\"[{r['chunk_id']} | {r['source']} | score={r['fused_score']:.4f}]\\n{r['content']}\\n\\n\"\n",
        "\n",
        "    prompt = rag_prompt.format(evidence=ev, query=query)\n",
        "    output = llm(prompt, max_length=1200, do_sample=False)[0][\"generated_text\"]\n",
        "    return output\n",
        "\n",
        "# ============================================================\n",
        "# ---------------------- DEMO --------------------------------\n",
        "# ============================================================\n",
        "\n",
        "docs = ingest_document_set(\n",
        "    pdf_paths=[r'/content/Data Scientist Case Study.pdf'],\n",
        "    image_paths=[],\n",
        "    table_paths=[]\n",
        ")\n",
        "\n",
        "print(f\"Total documents ingested: {len(docs)}\")\n",
        "print(docs[0] if docs else \"No documents found.\")\n",
        "\n",
        "# Build chunks\n",
        "all_chunks = []\n",
        "for doc in docs:\n",
        "    all_chunks.extend(chunk_text(doc[\"text\"], doc[\"source\"]))\n",
        "    for table in doc[\"tables\"]:\n",
        "        all_chunks.extend(chunk_table(table, doc[\"source\"]))\n",
        "    for img_text in doc[\"images_ocr\"]:\n",
        "        all_chunks.extend(chunk_image_text(img_text, doc[\"source\"]))\n",
        "\n",
        "print(f\"Total chunks created: {len(all_chunks)}\")\n",
        "\n",
        "# Build indexes\n",
        "dense_db = initialize_dense_index(\"enterprise_dense\")\n",
        "dense_db = build_dense_index(dense_db, all_chunks)\n",
        "bm25 = initialize_bm25(all_chunks)\n",
        "\n",
        "print(\"Hybrid indexes ready.\")\n",
        "\n",
        "# Queries\n",
        "queries = [\n",
        "    \"Summarize key product specifications.\",\n",
        "    \"List numerical data in the report.\",\n",
        "    \"Identify risks highlighted in the documents.\",\n",
        "    \"Provide overall insights.\"\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    output = run_agent(\n",
        "        query=query,\n",
        "        chunks=all_chunks,\n",
        "        dense_db=dense_db,\n",
        "        bm25=bm25,\n",
        "        k=5\n",
        "    )\n",
        "    print(output)\n",
        "    print(\"=\"*60, \"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
